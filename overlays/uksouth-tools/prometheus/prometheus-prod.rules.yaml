groups:
- name: prod
  rules:
  - alert: prod postgres active connections
    expr: (active_connections_count_average{resource_name=~"bink-uksouth-prod.*"} OR on() vector(0)) < 1
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.resource_name }}"
      summary: "Check Postgres {{ $labels.resource_name }}, reporting no current active connections"

  - alert: prod postgres cpu
    expr: cpu_percent_percent_max{resource_name=~"bink-uksouth-prod-.+"} > 90
    for: 15m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.resource_name }}"
      summary: Check prod postgres - CPU over 90% for 15 minutes

  - alert: prod postgres storage
    expr: storage_percent_percent_max{resource_name=~"bink-uksouth-prod-.+"} > 90
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.resource_name }}"
      summary: Check prod postgres - storage over 90% for 5 minutes

  - alert: prod postgres up state - direct
    expr: postgres_up_state{instance="bink-uksouth-prod"} < 1
    for: 3m
    labels:
      severity: S1
    annotations:
      resource: "{{ $labels.instance }}"
      summary: Check prod postgres flexible server - bad upstate for over 3 minutes
      
  - alert: prod postgres up state - pgbouncer
    expr: pgbouncer_up_state{instance="bink-uksouth-prod"} < 1
    for: 3m
    labels:
      severity: S1
    annotations:
      resource: "{{ $labels.instance }}"
      summary: Check prod postgres flexible server pgbouncer - bad upstate for over 3 minutes      

  - alert: prod rabbitmq virtual machine up state
    expr: (up{job="azure_node",name=~"prod-rabbitmq\\d"} OR on() vector(0)) < 1
    for: 3m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.name }}"
      summary: Check prod RabbitMQ virtual machines - bad up state for over 3 minutes

  - alert: prod rabbitmq virtual machine cluster depleted
    expr: sum(up{job="azure_node",name=~"prod-rabbitmq\\d"} OR on() vector(0)) < 2
    for: 3m
    labels:
      severity: S1
    annotations:
      resource: "{{ $labels.name }}"
      summary: Check prod RabbitMQ virtual machines - less than 2 virtual machines in the cluster for over 3 minutes

  - alert: prod rabbitmq running state
    expr: (rabbitmq_running{kubernetes_cluster=~"prod\\d",job="rabbitmq-exporter"} OR on() vector(0)) < 1
    for: 3m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.node }}"
      summary: Check prod RabbitMQ cluster - at least 1 RabbitMQ instance has not been running for over 3 minutes

  - alert: prod rabbitmq cluster depleted (prod0)
    expr: sum(rabbitmq_running{kubernetes_cluster="prod0",job="rabbitmq-exporter"} OR on() vector(0)) < 2
    for: 3m
    labels:
      severity: S1
    annotations:
      resource: "{{ $labels.node }}"
      summary: Check prod RabbitMQ cluster - tested from cluster prod0, less than 2 RabbitMQ instances have been running for over 3 minutes

  - alert: prod rabbitmq cluster depleted (prod1)
    expr: sum(rabbitmq_running{kubernetes_cluster="prod1",job="rabbitmq-exporter"} OR on() vector(0)) < 2
    for: 3m
    labels:
      severity: S1
    annotations:
      resource: "{{ $labels.node }}"
      summary: Check prod RabbitMQ cluster - tested from cluster prod1, less than 2 RabbitMQ instances have been running for over 3 minutes

  - alert: prod redis cpu
    expr: serverload_percent_max{resource_name="bink-uksouth-prod"} > 90
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.resource_name }}"
      summary: Check prod redis - CPU over 90% for over 5 minutes

  - alert: prod redis memory
    expr: (usedmemory_bytes_max{resource_name="bink-uksouth-prod"} / 1000000000) > 5.5
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.resource_name }}"
      summary: Check prod redis - memory 5.5G out of 6G used for over 5 minutes

  - alert: prod redis up state
    expr: (redis_up{instance="bink-uksouth-prod"} OR on() vector(0)) < 1
    for: 3m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.instance }}"
      summary: Check prod redis - bad upstate for over 3 minutes

  - alert: prod hermes - http response codes - more than five 400 codes in 2 mins
    expr: sum(rate(django_http_responses_total_by_status_view_method_total{status=~"4\\d\\d", kubernetes_cluster=~"prod\\d"} [2m])) > 5
    for: 1m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.view }} - {{$labels.status }}"
      summary: More than five 400 Http responses returning in 1 min
  
  - alert: prod hermes - http response codes - more than five 500 codes in 2 mins
    expr: sum(rate(django_http_responses_total_by_status_view_method_total{status=~"5\\d\\d", kubernetes_cluster=~"prod\\d"} [2m])) > 5
    for: 1m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.view }} - {{$labels.status }}"
      summary: More than five 500 Http responses returning in 1 min

  - alert: prod hermes - no 2xx barclays traffic detected for over 10 minutes (8am - 8pm only)
    expr: (sum(rate(django_http_responses_total_by_status_view_method_total{channel="com.barclays.bmb", kubernetes_cluster=~"prod\\d", status=~"2\\d\\d"} [5m])) OR on() vector(0)) == 0 and on() europe_london_hour >= 8 < 20
    for: 10m
    labels:
      severity: S2
    annotations:
      summary: No 2XX codes have returned for Barclays for 10 minutes

  - alert: prod hermes - no 2xx barclays traffic detected for over 1 hour
    expr: (sum(rate(django_http_responses_total_by_status_view_method_total{channel="com.barclays.bmb", kubernetes_cluster=~"prod\\d", status=~"2\\d\\d"} [5m])) OR on() vector(0)) == 0 
    for: 1h
    labels:
      severity: S1
    annotations:
      summary: No 2XX codes have returned for Barclays for 1 hour

  - alert: prod iris - increase in 4xx responces
    expr: sum(rate(iris_response_by_status_total{kubernetes_cluster=~"prod\\d", status=~"4\\d\\d"}[10m])) by (status) > 10
    for: 1m
    labels:
      severity: S2
    annotations:
      summary: There has been 10 4xx responses from iris in 10 mins

  - alert: prod iris - increase in 5xx responces
    expr: sum(rate(iris_response_by_status_total{kubernetes_cluster=~"prod\\d", status=~"5\\d\\d"}[10m])) by (status) > 10
    for: 1m
    labels:
      severity: S2
    annotations:
      summary: There has been 10 5xx responces from iris in 10 mins

  - alert: prod midas - failed http requests to merchants
    expr: sum(increase(request_fail_total{kubernetes_cluster=~"prod\\d"}[1h])) by (error, slug) > 6
    for: 90m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.slug }} - {{ $labels.error }}"
      summary: "There has been an increase in failed HTTP requests from Midas to this merchant"

  - alert: prod midas - failed login is increasing
    expr: sum(increase(log_in_fail_total{kubernetes_cluster=~"prod\\d"}[10m])) by (slug) > 5
    for: 15m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.slug }}"
      summary: There has been an increase of 5 in failed logins in 10 mins

  - alert: prod midas - failed registration events
    expr: sum(increase(join_fail_total{kubernetes_cluster=~"prod\\d"}[10m])) by (slug) > 5
    for: 1m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.slug }}"
      summary: There has been an increase on 5 failed registrations in the last 10 mins

  - alert: prod kube controller up state
    expr: (up{job="azure_node",name="prod0-controller"} OR on() vector(0)) < 1
    for: 5m
    labels:
      severity: S1
    annotations:
      resource: "{{ $labels.name }}"
      summary: Check virtual machine controller - bad up state for over 5 minutes

  - alert: prod kube worker up state
    expr: (up{job="azure_node",name=~"prod\\d-vmss_.+"} OR on() vector(0)) < 1
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.name }}"
      summary: Check virtual machine worker - bad up state for over 5 minutes

  - alert: prod kube worker pool depleted
    expr: (sum(up{job="azure_node",name=~"prod\\d-vmss_.+"}) OR on() vector(0)) < 7
    for: 3m
    labels:
      severity: S1
    annotations:
      summary: Check prod clusters - worker count less than 7

  - alert: prod virtual machines cpu
    expr: 100 - (avg by (name) (irate(node_cpu_seconds_total{job="azure_node",name=~"prod0-.+",mode="idle"}[5m])) * 100) > 90
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.name }}"
      summary: Check virtual machine - CPU over 90% for over 5 minutes

  - alert: prod virtual machines memory
    expr: node_memory_MemAvailable_bytes{job="azure_node",name=~"prod0-.+"} < 100000000
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.name }}"
      summary: Check virtual machine - Available memory < 100MB for over 5 minutes

  - alert: prod virtual machines root filesystem
    expr: 100 - ((node_filesystem_avail_bytes{job="azure_node",name=~"prod0-.+",mountpoint="/"} * 100) / node_filesystem_size_bytes{job="azure_node",name=~"prod0-.+",mountpoint="/"}) > 90
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.name }}"
      summary: Check virtual machines - Root filesystem over 90% used for over 5 minutes
