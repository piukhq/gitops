groups:
- name: prod
  rules:
  - alert: prod media backup tar file size
    expr: (100 * (1 - ((backup_utility_file_size{kubernetes_cluster="prod0",type="media"} offset 24h) / (backup_utility_file_size{kubernetes_cluster="prod0",type="media"}))) < -50) or (100 * (1 - ((backup_utility_file_size{kubernetes_cluster="prod0",type="media"} offset 24h) / (backup_utility_file_size{kubernetes_cluster="prod0",type="media"}))) > 50)
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.name }}"
      summary: Raw prod media backup file size has increased/decreased by over 50% within an 8 day window

  - alert: prod media backup uploads (weekly)
    expr: sum(increase(backup_utility_upload_to_blob_count_total{kubernetes_cluster=~"prod\\d",type="media"}[8d])) < 1
    for: 30d
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.type }} - {{ $labels.schedule }}"
      summary: Backup tar file failed to upload from backup-scheduler pod to binkuksouthprod/backups storage account

  - alert: prod postgres backup failure
    expr: sum(increase(backup_utility_file_count_total{job="backup-scheduler",resource=~"bink-uksouth-prod-.+",status="failed",type="postgres"}[61m])) > 1
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.name }} - {{ $labels.schedule }}"
      summary: Check prod backup-scheduler pod and binkuksouthprod/backups storage account - potential backup failure

  - alert: prod postgres backup sql file size
    expr: (100 * (1 - ((backup_utility_file_size{resource=~"bink-uksouth-prod-.+"} offset 24h) / (backup_utility_file_size{resource=~"bink-uksouth-prod-.+"}))) < -50) or (100 * (1 - ((backup_utility_file_size{resource=~"bink-uksouth-prod-.+"} offset 24h) / (backup_utility_file_size{resource=~"bink-uksouth-prod-.+"}))) > 50)
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.name }}"
      summary: Raw prod postgres backup file size has increased/decreased by over 50% within 24 hour window

  - alert: prod postgres backup uploads (hourly)
    expr: sum(increase(backup_utility_upload_to_blob_count_total{resource="binkuksouthprod",schedule="hourly",type="postgres"}[61m])) < 1
    for: 130m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.type }} - {{ $labels.schedule }}"
      summary: Backup tar file failed to upload from backup-scheduler pod to binkuksouthprod/backups storage account

  - alert: prod postgres backup uploads (weekly)
    expr: sum(increase(backup_utility_upload_to_blob_count_total{resource="binkuksouthprod",schedule="weekly",type="postgres"}[8d])) < 1
    for: 30d
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.type }} - {{ $labels.schedule }}"
      summary: Backup tar file failed to upload from backup-scheduler pod to binkuksouthprod/backups storage account

  - alert: prod postgres cpu
    expr: cpu_percent_percent_max{resource_name=~"bink-uksouth-prod-.+"} > 90
    for: 15m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.resource_name }}"
      summary: Check prod postgres - CPU over 90% for 15 minutes

  - alert: prod postgres storage
    expr: storage_percent_percent_max{resource_name=~"bink-uksouth-prod-.+"} > 90
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.resource_name }}"
      summary: Check prod postgres - storage over 90% for 5 minutes

  - alert: prod postgres common up state
    expr: (up{job="postgres",instance="bink-uksouth-prod-common"} OR on() vector(0)) < 1
    for: 3m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.instance }}"
      summary: Check prod postgres - bad upstate for over 3 minutes

  - alert: prod postgres hades up state
    expr: (up{job="postgres",instance="bink-uksouth-prod-hades"} OR on() vector(0)) < 1
    for: 3m
    labels:
      severity: S1
    annotations:
      resource: "{{ $labels.instance }}"
      summary: Check prod postgres - bad upstate for over 3 minutes

  - alert: prod postgres harmonia up state
    expr: (up{job="postgres",instance="bink-uksouth-prod-harmonia"} OR on() vector(0)) < 1
    for: 3m
    labels:
      severity: S1
    annotations:
      resource: "{{ $labels.instance }}"
      summary: Check prod postgres - bad upstate for over 3 minutes

  - alert: prod postgres hermes up state
    expr: (up{job="postgres",instance="bink-uksouth-prod-hermes"} OR on() vector(0)) < 1
    for: 3m
    labels:
      severity: S1
    annotations:
      resource: "{{ $labels.instance }}"
      summary: Check prod postgres - bad upstate for over 3 minutes

  - alert: prod rabbitmq virtual machine up state
    expr: (up{job="azure_node",name=~"prod-rabbitmq\\d"} OR on() vector(0)) < 1
    for: 3m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.name }}"
      summary: Check prod RabbitMQ virtual machines - bad up state for over 3 minutes

  - alert: prod rabbitmq virtual machine cluster depleted
    expr: sum(up{job="azure_node",name=~"prod-rabbitmq\\d"} OR on() vector(0)) < 2
    for: 3m
    labels:
      severity: S1
    annotations:
      resource: "{{ $labels.name }}"
      summary: Check prod RabbitMQ virtual machines - less than 2 virtual machines in the cluster for over 3 minutes

  - alert: prod rabbitmq running state
    expr: (rabbitmq_running{kubernetes_cluster=~"prod\\d",job="rabbitmq-exporter"} OR on() vector(0)) < 1
    for: 3m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.node }}"
      summary: Check prod RabbitMQ cluster - at least 1 RabbitMQ instance has not been running for over 3 minutes

  - alert: prod rabbitmq cluster depleted (prod0)
    expr: sum(rabbitmq_running{kubernetes_cluster="prod0",job="rabbitmq-exporter"} OR on() vector(0)) < 2
    for: 3m
    labels:
      severity: S1
    annotations:
      resource: "{{ $labels.node }}"
      summary: Check prod RabbitMQ cluster - tested from cluster prod0, less than 2 RabbitMQ instances have been running for over 3 minutes

  - alert: prod rabbitmq cluster depleted (prod1)
    expr: sum(rabbitmq_running{kubernetes_cluster="prod1",job="rabbitmq-exporter"} OR on() vector(0)) < 2
    for: 3m
    labels:
      severity: S1
    annotations:
      resource: "{{ $labels.node }}"
      summary: Check prod RabbitMQ cluster - tested from cluster prod1, less than 2 RabbitMQ instances have been running for over 3 minutes

  - alert: prod redis cpu
    expr: serverload_percent_max{resource_name=~"bink-uksouth-prod-.+"} > 90
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.resource_name }}"
      summary: Check prod redis - CPU over 90% for over 5 minutes

  - alert: prod redis memory
    expr: 100 * (usedmemory_bytes_max{resource_name=~"bink-uksouth-prod-.+"} / 1000000000) > 90
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.resource_name }}"
      summary: Check prod redis - memory over 90% for over 5 minutes

  - alert: prod redis common up state
    expr: (redis_up{instance="bink-uksouth-prod-common"} OR on() vector(0)) < 1
    for: 3m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.instance }}"
      summary: Check prod redis - bad upstate for over 3 minutes

  - alert: prod redis harmonia up state
    expr: (redis_up{instance="bink-uksouth-prod-harmonia"} OR on() vector(0)) < 1
    for: 3m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.instance }}"
      summary: Check prod redis - bad upstate for over 3 minutes

  - alert: prod harmonia - no exported transactions to iceland between 15:00 and 18:00
    expr: sum(increase(transactions_total{kubernetes_cluster="prod0", slug="iceland-bonus-card"} [12h])) by (slug) < 1 and europe_london_hour != 15 < 18
    for: 1m
    labels: 
      severity: S2
    annotations:
      resource: "{{ $labels.slug }}"
      summary: There have not been any exports to Iceland between 15:00 and 18:00
  
  - alert: prod harmonia - no exported transactions between 12:00 and 23:00
    expr: sum(increase(transactions_total{kubernetes_cluster=~"prod\\d", slug!="iceland-bonus-card"} [2h])) by (slug) < 1 and europe_london_hour != 12 < 23
    for: 1m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.slug }}"
      sumamry: There have been no exports between 12:00 and 23:00

  - alert: prod hermes - http response codes - more than five 400 codes in 2 mins
    expr: sum(rate(django_http_responses_total_by_status_view_method_total{status=~"4\\d\\d", kubernetes_cluster=~"prod\\d"} [2m])) > 5
    for: 1m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.view }} - {{$labels.status }}"
      summary: More than five 400 Http responses returning in 1 min
  
  - alert: prod hermes - http response codes - more than five 500 codes in 2 mins
    expr: sum(rate(django_http_responses_total_by_status_view_method_total{status=~"5\\d\\d", kubernetes_cluster=~"prod\\d"} [2m])) > 5
    for: 1m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.view }} - {{$labels.status }}"
      summary: More than five 500 Http responses returning in 1 min

  - alert: prod hermes - no 2xx barclays traffic detected for over 10 minutes (8am - 8pm only)
    expr: (sum(rate(django_http_responses_total_by_status_view_method_total{channel="com.barclays.bmb", kubernetes_cluster=~"prod\\d", status=~"2\\d\\d"} [5m])) OR on() vector(0)) == 0 and on() europe_london_hour >= 8 < 20
    for: 10m
    labels:
      severity: S2
    annotations:
      summary: No 2XX codes have returned for Barclays for 10 minutes

  - alert: prod hermes - no 2xx barclays traffic detected for over 1 hour
    expr: (sum(rate(django_http_responses_total_by_status_view_method_total{channel="com.barclays.bmb", kubernetes_cluster=~"prod\\d", status=~"2\\d\\d"} [5m])) OR on() vector(0)) == 0 
    for: 1h
    labels:
      severity: S1
    annotations:
      summary: No 2XX codes have returned for Barclays for 1 hour

  - alert: prod hermes - payment cards processing slow
    expr: (sum(payment_card_processing_seconds_histogram_sum{kubernetes_cluster=~"prod\\d"}) by (provider) / sum(payment_card_processing_seconds_histogram_count{kubernetes_cluster=~"prod\\d"}) by (provider)) > 5
    for: 5m
    labels:
      severity: S2
    annotations:
      summary: Payment cards for {{ $labels.provider }} are taking longer than four seconds to process.

  - alert: prod iris - increase in 4xx responces
    expr: sum(rate(iris_response_by_status_total{kubernetes_cluster=~"prod\\d", status=~"4\\d\\d"}[10m])) by (status) > 10
    for: 1m
    labels:
      severity: S2
    annotations:
      summary: There has been 10 4xx responses from iris in 10 mins

  - alert: prod iris - increase in 5xx responces
    expr: sum(rate(iris_response_by_status_total{kubernetes_cluster=~"prod\\d", status=~"5\\d\\d"}[10m])) by (status) > 10
    for: 1m
    labels:
      severity: S2
    annotations:
      summary: There has been 10 5xx responces from iris in 10 mins

  - alert: prod midas - failed http requests to merchants
    expr: sum(increase(request_fail_total{kubernetes_cluster=~"prod\\d"}[1h])) by (error, slug) > 6
    for: 90m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.slug }} - {{ $labels.error }}"
      summary: "There has been an increase in failed HTTP requests from Midas to this merchant"

  - alert: prod midas - failed login is increasing
    expr: sum(increase(log_in_fail_total{kubernetes_cluster=~"prod\\d"}[10m])) by (slug) > 5
    for: 15m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.slug }}"
      summary: There has been an increase of 5 in failed logins in 10 mins

  - alert: prod midas - failed registration events
    expr: sum(increase(join_fail_total{kubernetes_cluster=~"prod\\d"}[10m])) by (slug) > 5
    for: 1m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.slug }}"
      summary: There has been an increase on 5 failed registrations in the last 10 mins

  - alert: prod kube controller up state
    expr: (up{job="azure_node",name="prod0-controller"} OR on() vector(0)) < 1
    for: 5m
    labels:
      severity: S1
    annotations:
      resource: "{{ $labels.name }}"
      summary: Check virtual machine controller - bad up state for over 5 minutes

  - alert: prod kube worker up state
    expr: (up{job="azure_node",name=~"prod\\d-vmss_.+"} OR on() vector(0)) < 1
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.name }}"
      summary: Check virtual machine worker - bad up state for over 5 minutes

  - alert: prod kube worker pool depleted
    expr: (sum(up{job="azure_node",name=~"prod\\d-vmss_.+"}) OR on() vector(0)) < 7
    for: 3m
    labels:
      severity: S1
    annotations:
      summary: Check prod clusters - worker count less than 7

  - alert: prod virtual machines cpu
    expr: 100 - (avg by (name) (irate(node_cpu_seconds_total{job="azure_node",name=~"prod0-.+",mode="idle"}[5m])) * 100) > 90
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.name }}"
      summary: Check virtual machine - CPU over 90% for over 5 minutes

  - alert: prod virtual machines memory
    expr: node_memory_MemAvailable_bytes{job="azure_node",name=~"prod0-.+"} < 100000000
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.name }}"
      summary: Check virtual machine - Available memory < 100MB for over 5 minutes

  - alert: prod virtual machines root filesystem
    expr: 100 - ((node_filesystem_avail_bytes{job="azure_node",name=~"prod0-.+",mountpoint="/"} * 100) / node_filesystem_size_bytes{job="azure_node",name=~"prod0-.+",mountpoint="/"}) > 90
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.name }}"
      summary: Check virtual machines - Root filesystem over 90% used for over 5 minutes
