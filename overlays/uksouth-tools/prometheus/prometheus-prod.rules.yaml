groups:
- name: prod
  rules:
  - alert: prod media backup tar file size
    expr: (100 * (1 - ((backup_utility_file_size{kubernetes_cluster="prod0",type="media"} offset 24h) / (backup_utility_file_size{kubernetes_cluster="prod0",type="media"}))) < -50) or (100 * (1 - ((backup_utility_file_size{kubernetes_cluster="prod0",type="media"} offset 24h) / (backup_utility_file_size{kubernetes_cluster="prod0",type="media"}))) > 50)
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.name }}"
      summary: Raw prod media backup file size has increased/decreased by over 50% within an 8 day window

  - alert: prod media backup uploads (weekly)
    expr: (increase(backup_utility_upload_to_blob_count_total{kubernetes_cluster="prod0",type="media"}[8d]) OR on() vector(0)) < 1
    for: 8d
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.type }} - {{ $labels.schedule }}"
      summary: Backup tar file failed to upload from backup-scheduler pod to binkuksouthprod/backups storage account

  - alert: prod postgres backup failure
    expr: increase(backup_utility_file_count_total{job="backup-scheduler",resource=~"bink-uksouth-prod-.+",status="failed",type="postgres"}[61m]) > 1
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.name }} - {{ $labels.schedule }}"
      summary: Check prod backup-scheduler pod and binkuksouthprod/backups storage account - potential backup failure

  - alert: prod postgres backup sql file size
    expr: (100 * (1 - ((backup_utility_file_size{resource=~"bink-uksouth-prod-.+"} offset 24h) / (backup_utility_file_size{resource=~"bink-uksouth-prod-.+"}))) < -50) or (100 * (1 - ((backup_utility_file_size{resource=~"bink-uksouth-prod-.+"} offset 24h) / (backup_utility_file_size{resource=~"bink-uksouth-prod-.+"}))) > 50)
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.name }}"
      summary: Raw prod postgres backup file size has increased/decreased by over 50% within 24 hour window

  - alert: prod postgres backup uploads (hourly)
    expr: (increase(backup_utility_upload_to_blob_count_total{resource="binkuksouthprod",schedule="hourly",type="postgres"}[61m]) OR on() vector(0)) < 1
    for: 90m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.type }} - {{ $labels.schedule }}"
      summary: Backup tar file failed to upload from backup-scheduler pod to binkuksouthprod/backups storage account

  - alert: prod postgres cpu
    expr: cpu_percent_percent_max{resource_name=~"bink-uksouth-prod-.+"} > 90
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.resource_name }}"
      summary: Check prod postgres - CPU over 90% for 5 minutes

  - alert: prod postgres storage
    expr: storage_percent_percent_max{resource_name=~"bink-uksouth-prod-.+"} > 90
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.resource_name }}"
      summary: Check prod postgres - storage over 90% for 5 minutes

  - alert: prod postgres common up state
    expr: (up{job="postgres",instance="bink-uksouth-prod-common"} OR on() vector(0)) < 1
    for: 3m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.instance }}"
      summary: Check prod postgres - bad upstate for over 3 minutes

  - alert: prod postgres hades up state
    expr: (up{job="postgres",instance="bink-uksouth-prod-hades"} OR on() vector(0)) < 1
    for: 3m
    labels:
      severity: S1
    annotations:
      resource: "{{ $labels.instance }}"
      summary: Check prod postgres - bad upstate for over 3 minutes

  - alert: prod postgres harmonia up state
    expr: (up{job="postgres",instance="bink-uksouth-prod-harmonia"} OR on() vector(0)) < 1
    for: 3m
    labels:
      severity: S1
    annotations:
      resource: "{{ $labels.instance }}"
      summary: Check prod postgres - bad upstate for over 3 minutes

  - alert: prod postgres hermes up state
    expr: (up{job="postgres",instance="bink-uksouth-prod-hermes"} OR on() vector(0)) < 1
    for: 3m
    labels:
      severity: S1
    annotations:
      resource: "{{ $labels.instance }}"
      summary: Check prod postgres - bad upstate for over 3 minutes

  - alert: prod rabbitmq virtual machine up state
    expr: (up{job="azure_node",name=~"prod-rabbitmq\\d"} OR on() vector(0)) < 1
    for: 3m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.name }}"
      summary: Check prod RabbitMQ virtual machines - bad up state for over 3 minutes

  - alert: prod rabbitmq virtual machine cluster depleted
    expr: sum(up{job="azure_node",name=~"prod-rabbitmq\\d"} OR on() vector(0)) < 2
    for: 3m
    labels:
      severity: S1
    annotations:
      resource: "{{ $labels.name }}"
      summary: Check prod RabbitMQ virtual machines - less than 2 virtual machines in the cluster for over 3 minutes

  - alert: prod rabbitmq running state
    expr: (rabbitmq_running{kubernetes_cluster="prod0",job="rabbitmq-exporter"} OR on() vector(0)) < 1
    for: 3m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.node }}"
      summary: Check prod RabbitMQ cluster - at least 1 RabbitMQ instance has not been running for over 3 minutes

  - alert: prod rabbitmq cluster depleted
    expr: sum(rabbitmq_running{kubernetes_cluster="prod0",job="rabbitmq-exporter"} OR on() vector(0)) < 2
    for: 3m
    labels:
      severity: S1
    annotations:
      resource: "{{ $labels.node }}"
      summary: Check prod RabbitMQ cluster - less than 2 RabbitMQ instances have been running for over 3 minutes

  - alert: prod redis cpu
    expr: serverload_percent_max{resource_name=~"bink-uksouth-prod-.+"} > 90
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.resource_name }}"
      summary: Check prod redis - CPU over 90% for over 5 minutes

  - alert: prod redis memory
    expr: 100 * (usedmemory_bytes_max{resource_name=~"bink-uksouth-prod-.+"} / 1000000000) > 90
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.resource_name }}"
      summary: Check prod redis - memory over 90% for over 5 minutes

  - alert: prod redis common up state
    expr: (redis_up{instance="bink-uksouth-prod-common"} OR on() vector(0)) < 1
    for: 3m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.instance }}"
      summary: Check prod redis - bad upstate for over 3 minutes

  - alert: prod redis harmonia up state
    expr: (redis_up{instance="bink-uksouth-prod-harmonia"} OR on() vector(0)) < 1
    for: 3m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.instance }}"
      summary: Check prod redis - bad upstate for over 3 minutes

  - alert: prod harmonia - failed exports not including receipt not found
    expr: sum(failed_requests_total{kubernetes_cluster="prod0", response_result!="receipt no not found"}) > sum(failed_requests_total{kubernetes_cluster="prod0", response_result!="receipt no not found"} offset 15m)
    for: 1m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.view }} - {{ $labels.status }} - {{ $labels.slug }} - {{ $labels.response_result }}"
      summary: There have been failed exports from harmonia.
  
  - alert: prod harmonia - failed exports for receipt not found
    expr: sum(failed_requests_total{kubernetes_cluster="prod0", response_result="receipt no not found"}) > sum(failed_requests_total{kubernetes_cluster="prod0", response_result="receipt no not found"} offset 15m) + 10
    for: 1m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.view }} - {{ $labels.status }} - {{ $labels.slug }} - {{ $labels.response_result }}"
      summary: There have been 10 or more failed exports returning receipt not found.

  - alert: prod harmonia - no exported transactions to iceland between 15:00 and 18:00
    expr: sum(increase(transactions_total{kubernetes_cluster="prod0", slug="iceland-bonus-card"} [12h])) by (slug) < 1 and europe_london_hour != 15 < 18
    for: 1m
    labels: 
      severity: S2
    annotations:
      resource: "{{ $labels.slug }}"
      summary: There have not been any exports to Iceland between 15:00 and 18:00
  
  - alert: prod harmonia - no exported transactions between 12:00 and 23:00
    expr: sum(increase(transactions_total{kubernetes_cluster=~"prod\\d", slug!="iceland-bonus-card"} [2h])) by (slug) < 1 and europe_london_hour != 12 < 23
    for: 1m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.slug }}"
      sumamry: There have been no exports between 12:00 and 23:00

  - alert: prod hermes - http response codes - more than five 400 codes in 2 mins
    expr: sum(rate(django_http_responses_total_by_status_view_method_total{status=~"4\\d\\d", kubernetes_cluster=~"prod\\d"} [2m])) > 5
    for: 1m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.view }} - {{$labels.status }}"
      summary: More than five 400 Http responses returning in 1 min
  
  - alert: prod hermes - http response codes - more than five 500 codes in 2 mins
    expr: sum(rate(django_http_responses_total_by_status_view_method_total{status=~"5\\d\\d", kubernetes_cluster=~"prod\\d"} [2m])) > 5
    for: 1m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.view }} - {{$labels.status }}"
      summary: More than five 500 Http responses returning in 1 min

  - alert: prod hermes - http response latency - greater than 1 second
    expr: histogram_quantile(0.99, sum by(view, le)  (rate(django_http_requests_latency_seconds_by_view_method_bucket{job="hermes", view=~"membership-plan.*|service|membership-card.*|payment-card.*|register_user|login", kubernetes_cluster=~"prod\\d"}[2m]))) > 1
    for: 3m
    labels:
      severity: S2 - Monitoring
    annotations:
      resource: "{{ $labels.view }}"
      summary: Http responses are returning is over a second for more than 3 minutes
  
  - alert: prod hermes - http response latency - greater than 5 second
    expr: histogram_quantile(0.99, sum by(view, le)  (rate(django_http_requests_latency_seconds_by_view_method_bucket{job="hermes", view=~"membership-plan.*|service|membership-card.*|payment-card.*|register_user|login", kubernetes_cluster=~"prod\\d"}[2m]))) > 5
    for: 3m
    labels:
      severity: S2 - Monitoring
    annotations:
      resource: "{{ $labels.view }}"
      summary: Http responses are returning is over five seconds for more than 3 minutes

  - alert: prod hermes - no 2xx barclays traffic detected for over 10 minutes (8am - 8pm only)
    expr: sum(rate(django_http_responses_total_by_status_view_method_total{channel="com.barclays.bmb", kubernetes_cluster=~"prod\\d", status=~"2\\d\\d"} [5m])) OR on() vector(0)) == 0 and on() europe_london_hour >= 8 < 20
    for: 10m
    labels:
      severity: S2
    annotations:
      summary: No 2XX codes have returned for Barclays for 10 minutes

  - alert: prod hermes - no 2xx barclays traffic detected for over 1 hour
    expr: sum(rate(django_http_responses_total_by_status_view_method_total{channel="com.barclays.bmb", kubernetes_cluster=~"prod\\d", status=~"2\\d\\d"} [5m])) OR on() vector(0)) == 0 
    for: 1h
    labels:
      severity: S1
    annotations:
      summary: No 2XX codes have returned for Barclays for 1 hour

  - alert: prod hermes - payment cards stuck increase
    expr: sum(hermes_current_payment_card_pending_overdue_total{kubernetes_cluster=~"prod\\d"}) > sum(hermes_current_payment_card_pending_overdue_total{kubernetes_cluster=~"prod\\d"} offset 24h +15)
    for: 5m
    labels:
      severity: S2 - Monitoring
    annotations:
      summary: Payment cards have been stuck in a pending state for more than 24 hours

  - alert: prod hermes - payment cards processing slow
    expr: sum(rate(payment_card_processing_seconds_histogram_bucket{kubernetes_cluster=~"prod\\d"}[2m])) by (le) > 0.3
    for: 5m
    labels:
      severity: S2
    annotations:
      summary: Payment cards are taking longer than 300ms to process in hermes
  
  - alert: prod hermes/midas - vop status stuck in transitory state
    expr: sum(hermes_current_vop_activation_status_total{kubernetes_cluster=~"prod\\d", status=~"Deactivating|Activating"}) > sum(hermes_current_vop_activation_status_total{kubernetes_cluster=~"prod\\d", status=~"Deactivating|Activating"} offset 15m)
    for: 3m
    labels:
      severity: S3 - Monitoring
    annotations:
      summary: "VOP account status in transitory state for over 3 minutes - check https://api.gb.bink.com/admin/periodic_retry/periodicretry/ and search midas logs for Payment Card ID in Kibana https://kibana.uksouth.bink.sh/"

  - alert: prod iris - increase in 4xx responces
    expr: sum(rate(iris_response_by_status_total{kubernetes_cluster=~"prod\\d", status=~"4\\d\\d"}[10m])) by (status) > 10
    for: 1m
    labels:
      severity: S2
    annotations:
      summary: There has been 10 4xx responses from iris in 10 mins

  - alert: prod iris - increase in 5xx responces
    expr: sum(rate(iris_response_by_status_total{kubernetes_cluster=~"prod\\d", status=~"5\\d\\d"}[10m])) by (status) > 10
    for: 1m
    labels:
      severity: S2
    annotations:
      summary: There has been 10 5xx responces from iris in 10 mins

  - alert: prod harmonia - harvey nichols files not received
    expr: (blobstorage_files_total{type="scheme",name="harvey-nichols"} OR on() vector(0)) < 1
    for: 24h
    labels:
      severity: S2 - Monitoring
    annotations:
      resource: "{{ $labels.name }} - {{ $labels.type }}"
      summary: Harvey Nichols files haven't been received in over 24 hours

  - alert: prod harmonia - iceland files not received (monday - friday only)
    expr: (blobstorage_files_total{type="scheme",name="iceland"} OR on() vector(0)) < 1 and on() day_of_week() > 0 < 6
    for: 24h
    labels:
      severity: S2 - Monitoring
    annotations:
      resource: "{{ $labels.name }} - {{ $labels.type }}"
      summary: Iceland files haven't been received in over 24 hours

  - alert: prod harmonia - wasabi files not received (12pm - 8pm only)
    expr: (increase(blobstorage_files_total{name="wasabihack"}[46m]) or on() vector(0)) < 1 and on() europe_london_hour >= 12 < 20
    for: 3m
    labels:
      severity: S2 - Monitoring
    annotations:
      resource: "{{ $labels.name }}"
      summary: Three missing files over 45 minute period - check latest Wasabi import (Transaction log lands every 15 minutes between the hours of 12pm and 8pm)

  - alert: prod midas - failed callbacks too merchant
    expr: sum(increase(callback_fail_total{kubernetes_cluster=~"prod\\d"} [10m])) by (slug) > 5
    for: 1m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.slug }}"
      summary: There has been an increase of 5 failed callbacks in the last 10 mins

  - alert: prod midas - failed http requests to merchants
    expr: sum(increase(request_fail_total{kubernetes_cluster=~"prod\\d"}[1h])) by (error, slug) > 6
    for: 90m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.slug }} - {{ $labels.error }}"
      summary: "There has been an increase in failed HTTP requests from Midas to this merchant"

  - alert: prod midas - failed login is increasing
    expr: sum(increase(log_in_fail_total{kubernetes_cluster=~"prod\\d"}[10m])) by (slug) > 5
    for: 15m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.slug }}"
      summary: There has been an increase of 5 in failed logins in 10 mins

  - alert: prod midas - failed registration events
    expr: sum(increase(register_fail_total{kubernetes_cluster=~"prod\\d"}[10m])) by (slug) > 5
    for: 1m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.slug }}"
      summary: There has been an increase on 5 failed registrations in the last 10 mins

  - alert: prod harmonia - amex files not received
    expr: (blobstorage_files_total{type="payment", name="amex"} OR on() vector(0)) < 1
    for: 24h
    labels:
      severity: S2 - Monitoring
    annotations:
      resource: "{{ $labels.name }} - {{ $labels.type }}"
      summary: Payment files for amex haven't been received in over 24 hours
  
  - alert: prod harmonia - mastercard files not received
    expr: (blobstorage_files_total{type="payment", name="mastercard"} OR on() vector(0)) < 1
    for: 27h
    labels:
      severity: S2 - Monitoring
    annotations:
      resource: "{{ $labels.name }} - {{ $labels.type }}"
      summary: Payment files for mastercard haven't been received in over 27 hours

  - alert: prod kube controller up state
    expr: (up{job="azure_node",name="prod0-controller"} OR on() vector(0)) < 1
    for: 5m
    labels:
      severity: S1
    annotations:
      resource: "{{ $labels.name }}"
      summary: Check virtual machine controller - bad up state for over 5 minutes

  - alert: prod kube worker up state
    expr: (up{job="azure_node",name=~"prod\\d-vmss_.+"} OR on() vector(0)) < 1
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.name }}"
      summary: Check virtual machine worker - bad up state for over 5 minutes

  - alert: prod kube worker pool depleted
    expr: (sum(up{job="azure_node",name=~"prod\\d-vmss_.+"}) OR on() vector(0)) < 7
    for: 3m
    labels:
      severity: S1
    annotations:
      summary: Check prod clusters - worker count less than 7

  - alert: prod virtual machines cpu
    expr: 100 - (avg by (name) (irate(node_cpu_seconds_total{job="azure_node",name=~"prod0-.+",mode="idle"}[5m])) * 100) > 90
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.name }}"
      summary: Check virtual machine - CPU over 90% for over 5 minutes

  - alert: prod virtual machines memory
    expr: node_memory_MemAvailable_bytes{job="azure_node",name=~"prod0-.+"} < 100000000
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.name }}"
      summary: Check virtual machine - Available memory < 100MB for over 5 minutes

  - alert: prod virtual machines root filesystem
    expr: 100 - ((node_filesystem_avail_bytes{job="azure_node",name=~"prod0-.+",mountpoint="/"} * 100) / node_filesystem_size_bytes{job="azure_node",name=~"prod0-.+",mountpoint="/"}) > 90
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.name }}"
      summary: Check virtual machines - Root filesystem over 90% used for over 5 minutes
  
