groups:
- name: daylight_saving
  rules:
  - record: is_european_summer_time
    expr: |
        (vector(1) and (month() > 3 and month() < 10))
        or
        (vector(1) and (month() == 3 and (day_of_month() - day_of_week()) >= 25) and absent((day_of_month() >= 25) and (day_of_week() == 0)))
        or
        (vector(1) and (month() == 10 and (day_of_month() - day_of_week()) < 25) and absent((day_of_month() >= 25) and (day_of_week() == 0)))
        or
        (vector(1) and ((month() == 10 and hour() < 1) or (month() == 3 and hour() > 0)) and ((day_of_month() >= 25) and (day_of_week() == 0)))
        or
        vector(0)
  - record: europe_london_hour
    expr: hour() + is_european_summer_time

- name: core
  rules:
  - alert: core ssl external (non-letsencrypt) 10 days
    expr: ((ssl_cert_not_after{cn!~".*(COMODO RSA Domain Validation|Sectigo RSA Domain Validation|RSA Certification Authority|Let's Encrypt Authority X3|R3).*",issuer_cn!~".*(Let's Encrypt|COMODO RSA Domain Validation|R3).*"} - time()) / 24 / 60 / 60) < 10
    labels:
      severity: S1
    annotations:
      resource: "{{ $labels.instance }}"
      summary: Cert less than 10 days from expiring

  - alert: core ssl external (non-letsencrypt) 30 days
    expr: ((ssl_cert_not_after{cn!~".*(COMODO RSA Domain Validation|Sectigo RSA Domain Validation|RSA Certification Authority|Let's Encrypt Authority X3|R3).*",issuer_cn!~".*(Let's Encrypt|COMODO RSA Domain Validation|R3).*"} - time()) / 24 / 60 / 60) < 30
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.instance }}"
      summary: Cert less than 30 days from expiring

  - alert: core ssl external (letsencrypt) 05 days
    expr: ((ssl_cert_not_after{cn!~".*(COMODO RSA Domain Validation|Sectigo RSA Domain Validation|RSA Certification Authority|Let's Encrypt Authority X3).*",issuer_cn=~".*(Let's Encrypt|R3).*"} - time())/24/60/60) < 5
    labels:
      severity: S1
    annotations:
      resource: "{{ $labels.instance }}"
      summary: Let's Encrypt cert less than 05 days from expiring

  - alert: core ssl external (letsencrypt) 10 days
    expr: ((ssl_cert_not_after{cn!~".*(COMODO RSA Domain Validation|Sectigo RSA Domain Validation|RSA Certification Authority|Let's Encrypt Authority X3).*",issuer_cn=~".*(Let's Encrypt|R3).*"} - time())/24/60/60) < 10
    labels:
      severity: S2
    annotations:
      summary: Let's Encrypt cert less than 10 days from expiring

  - alert: core ssl kubernetes internal (letsencrypt) 05 days
    expr: (certmanager_certificate_expiration_timestamp_seconds - time())/24/60/60 < 5
    labels:
      severity: S1
    annotations:
      resource: "{{ $labels.name }}"
      summary: Let's Encrypt Kubernetes cert less than 05 days from expiring

  - alert: core ssl kubernetes internal (letsencrypt) 10 days
    expr: (certmanager_certificate_expiration_timestamp_seconds - time())/24/60/60 < 10
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.name }}"
      summary: Let's Encrypt Kubernetes cert less than 10 days from expiring

  - alert: core test alerting and callout chain (should be red)
    expr: ({job="always_alert"} OR on() vector(0)) == 0
    for: 1m
    labels:
      severity: S3
    annotations:
      summary: This alert will test the Prometheus, Alertmanager, xMatters alerting chain at 10am every Friday. It should always be red.

  # This does not check AAAA record as we make no use of that.
  # - alert: azure frontdoor ip change
  #   expr: probe_success{job="azurefrontdoor_dns"} < 1
  #   for: 3m
  #   labels:
  #     severity: S2
  #   annotations:
  #     summary: Azure FrontDoor IP is not 13.107.246.19

  - alert: core aqua0 worker pool depleted
    expr: 100 * (sum(up{job="azure_node",name=~"aqua0-worker\\d\\d"}) / count(up{job="azure_node",name=~"aqua0-worker\\d\\d"})) <= 50
    for: 10m
    labels:
      severity: S2
    annotations:
      summary: Check aqua0 workers - pool depleted by at least 50%

  - alert: core confluence-macro up state
    expr: (up{job="confluence-macro",kubernetes_cluster="tools"} OR on() vector(0)) < 1
    for: 3m
    labels:
      severity: S2
    annotations:
      summary: Check confluence-macro (pod in uksouth-tools Kubernetes cluster) - bad up state

  - alert: core virtual machines cpu
    expr: 100 - (avg by (name) (irate(node_cpu_seconds_total{env="Core",name!~"aqua0-worker0\\d",job="azure_node",mode="idle"}[5m])) * 100) > 90
    for: 10m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.name }}"
      summary: Check virtual machine - CPU over 90% for over 10 minutes

  - alert: core virtual machines memory
    expr: node_memory_MemAvailable_bytes{job="azure_node",env="Core"} < 100000000
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.name }}"
      summary: Check virtual machine - Available memory < 100MB for over 5 minutes

  - alert: core virtual machines root filesystem
    expr: 100 - ((node_filesystem_avail_bytes{job="azure_node",env="Core",mountpoint="/",name!~"elasticsearch-\\d\\d"} * 100) / node_filesystem_size_bytes{job="azure_node",env="Core",mountpoint="/",name!~"elasticsearch-\\d\\d"}) > 90
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.name }}"
      summary: Check virtual machine - Root filesystem over 90% used for over 5 minutes

  - alert: core virtual machines up state
    expr: up{job="azure_node",env="Core",name!~"sftp\\d|chef-01|wordpress"} < 1
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.name }}"
      summary: Check virtual machine - bad up state for over 5 minutes

  - alert: core elasticsearch degraded
    expr: elasticsearch_cluster_health_status{color="yellow"} > 0
    for: 3m
    labels:
      severity: S2
    annotations:
      summary: Elasticsearch is degraded, monitor, check num(nodes) >= max(index_replica_count), should solve itself

  - alert: core elasticsearch is completely buggered
    expr: elasticsearch_cluster_health_status{color="red"} > 0
    for: 3m
    labels:
      severity: S2
    annotations:
      summary: Elasticsearch is broken, fix now.

  - alert: core elasticsearch free memory too log
    expr: elasticsearch_jvm_memory_used_bytes{area="heap"} / elasticsearch_jvm_memory_max_bytes{area="heap"} > 0.9
    for: 3m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.name }}"
      summary: Elasticsearch needs more RAM, add more RAM / nodes

  - alert: core elasticsearch root filesystem
    expr: 100 - ((node_filesystem_avail_bytes{job="azure_node",env="Core",mountpoint="/",name=~"elasticsearch-\\d\\d"} * 100) / node_filesystem_size_bytes{job="azure_node",env="Core",mountpoint="/",name=~"elasticsearch-\\d\\d"}) > 94
    for: 5m
    labels:
      severity: S2
    annotations:
      resource: "{{ $labels.name }}"
      summary: Check virtual machine - Root filesystem over 94% used for over 5 minutes
      
  - alert: core sftp up state
    expr: up{job="azure_node",name=~"sftp\\d"} < 1
    for: 3m
    labels:
      severity: S1
    annotations:
      resource: "{{ $labels.name }}"
      summary: Check sftp virtual machines - bad up state for over 3 minutes

  - alert: core tools0 virtual machines worker pool depleted
    expr: (sum(up{job="azure_node",name=~"tools0-vmss_.+"}) OR on() vector(0)) < 2
    for: 3m
    labels:
      severity: S2
    annotations:
      summary: Check tools0 cluster - worker count less than 2
