apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-server-conf
  namespace: default
  labels:
    name: prometheus-server-conf
data:
  prometheus.rules: |-
    groups:
    - name: daylight_saving
      rules:
      - record: is_european_summer_time
        expr: |
            (vector(1) and (month() > 3 and month() < 10))
            or
            (vector(1) and (month() == 3 and (day_of_month() - day_of_week()) >= 25) and absent((day_of_month() >= 25) and (day_of_week() == 0)))
            or
            (vector(1) and (month() == 10 and (day_of_month() - day_of_week()) < 25) and absent((day_of_month() >= 25) and (day_of_week() == 0)))
            or
            (vector(1) and ((month() == 10 and hour() < 1) or (month() == 3 and hour() > 0)) and ((day_of_month() >= 25) and (day_of_week() == 0)))
            or
            vector(0)
      - record: europe_london_hour
        expr: hour() + is_european_summer_time

    - name: certs
      rules:
      - alert: SSL External (non-letsencrypt) 10 Days
        expr: ((ssl_cert_not_after{cn!~".*(COMODO RSA Domain Validation|Sectigo RSA Domain Validation|RSA Certification Authority|Let's Encrypt Authority X3|R3).*",issuer_cn!~".*(Let's Encrypt|COMODO RSA Domain Validation|R3).*"} - time()) / 24 / 60 / 60) < 10
        labels:
          severity: P1
        annotations:
          resource: "{{ $labels.instance }}"
          summary: Cert less than 10 days from expiring

      - alert: SSL External (non-letsencrypt) 30 Days
        expr: ((ssl_cert_not_after{cn!~".*(COMODO RSA Domain Validation|Sectigo RSA Domain Validation|RSA Certification Authority|Let's Encrypt Authority X3|R3).*",issuer_cn!~".*(Let's Encrypt|COMODO RSA Domain Validation|R3).*"} - time()) / 24 / 60 / 60) < 30
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.instance }}"
          summary: Cert less than 30 days from expiring

      - alert: SSL External (letsencrypt) 05 Days
        expr: ((ssl_cert_not_after{cn!~".*(COMODO RSA Domain Validation|Sectigo RSA Domain Validation|RSA Certification Authority|Let's Encrypt Authority X3).*",issuer_cn=~".*(Let's Encrypt|R3).*"} - time())/24/60/60) < 5
        labels:
          severity: P1
        annotations:
          resource: "{{ $labels.instance }}"
          summary: Let's Encrypt cert less than 05 days from expiring

      - alert: SSL External (letsencrypt) 10 Days
        expr: ((ssl_cert_not_after{cn!~".*(COMODO RSA Domain Validation|Sectigo RSA Domain Validation|RSA Certification Authority|Let's Encrypt Authority X3).*",issuer_cn=~".*(Let's Encrypt|R3).*"} - time())/24/60/60) < 10
        labels:
          severity: P2
        annotations:
          summary: Let's Encrypt cert less than 10 days from expiring

      - alert: SSL Kubernetes Internal (letsencrypt) 05 Days
        expr: (certmanager_certificate_expiration_timestamp_seconds - time())/24/60/60 < 5
        labels:
          severity: P1
        annotations:
          resource: "{{ $labels.name }}"
          summary: Let's Encrypt Kubernetes cert less than 05 days from expiring

      - alert: SSL Kubernetes Internal (letsencrypt) 10 Days
        expr: (certmanager_certificate_expiration_timestamp_seconds - time())/24/60/60 < 10
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Let's Encrypt Kubernetes cert less than 10 days from expiring

    - name: core
      rules:
      # This does not check AAAA record as we make no use of that.
      # - alert: azure frontdoor ip change
      #   expr: probe_success{job="azurefrontdoor_dns"} < 1
      #   for: 3m
      #   labels:
      #     severity: P2
      #   annotations:
      #     summary: Azure FrontDoor IP is not 13.107.246.19

      - alert: aqua0 worker pool depleted
        expr: 100 * (sum(up{job="azure_node",name=~"aqua0-worker\\d\\d"}) / count(up{job="azure_node",name=~"aqua0-worker\\d\\d"})) <= 50
        for: 10m
        labels:
          severity: P2
        annotations:
          summary: Check aqua0 workers - pool depleted by at least 50%

      - alert: confluence-macro up state
        expr: (up{job="confluence-macro",kubernetes_cluster="tools"} OR on() vector(0)) < 1
        for: 3m
        labels:
          severity: P2
        annotations:
          summary: Check confluence-macro (pod in uksouth-tools Kubernetes cluster) - bad up state

      - alert: core virtual machines cpu
        expr: 100 - (avg by (name) (irate(node_cpu_seconds_total{env="Core",name!~"aqua0-worker0\\d",job="azure_node",mode="idle"}[5m])) * 100) > 90
        for: 10m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Check virtual machine - CPU over 90% for over 10 minutes

      - alert: core virtual machines memory
        expr: node_memory_MemAvailable_bytes{job="azure_node",env="Core"} < 100000000
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Check virtual machine - Available memory < 100MB for over 5 minutes

      - alert: core virtual machines root filesystem
        expr: 100 - ((node_filesystem_avail_bytes{job="azure_node",env="Core",mountpoint="/",name!~"elasticsearch-\\d\\d"} * 100) / node_filesystem_size_bytes{job="azure_node",env="Core",mountpoint="/",name!~"elasticsearch-\\d\\d"}) > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Check virtual machine - Root filesystem over 90% used for over 5 minutes

      - alert: core virtual machines up state
        expr: up{job="azure_node",env="Core",name!~"sftp\\d|chef-01|wordpress"} < 1
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Check virtual machine - bad up state for over 5 minutes

      - alert: Elasticsearch degraded
        expr: elasticsearch_cluster_health_status{color="yellow"} > 0
        for: 3m
        labels:
          severity: P2
        annotations:
          summary: Elasticsearch is degraded, monitor, check num(nodes) >= max(index_replica_count), should solve itself

      - alert: Elasticsearch is completely buggered
        expr: elasticsearch_cluster_health_status{color="red"} > 0
        for: 3m
        labels:
          severity: P2
        annotations:
          summary: Elasticsearch is broken, fix now.

      - alert: Elasticsearch Free Memory too log
        expr: elasticsearch_jvm_memory_used_bytes{area="heap"} / elasticsearch_jvm_memory_max_bytes{area="heap"} > 0.9
        for: 3m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Elasticsearch needs more RAM, add more RAM / nodes

      - alert: Elasticsearch root filesystem
        expr: 100 - ((node_filesystem_avail_bytes{job="azure_node",env="Core",mountpoint="/",name=~"elasticsearch-\\d\\d"} * 100) / node_filesystem_size_bytes{job="azure_node",env="Core",mountpoint="/",name=~"elasticsearch-\\d\\d"}) > 94
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Check virtual machine - Root filesystem over 94% used for over 5 minutes

      - alert: pypi up state
        expr: (up{job="pypi",kubernetes_cluster="tools"} OR on() vector(0)) < 1
        for: 3m
        labels:
          severity: P2
        annotations:
          summary: Check pypi (pod in uksouth-tools Kubernetes cluster) - bad up state
          
      - alert: sftp up state
        expr: up{job="azure_node",name=~"sftp\\d"} < 1
        for: 3m
        labels:
          severity: P1
        annotations:
          resource: "{{ $labels.name }}"
          summary: Check sftp virtual machines - bad up state for over 3 minutes

      - alert: tools worker pool depleted
        expr: 100 * (sum(up{job="azure_node",name=~"tools-worker-\\d\\d"}) / count(up{job="azure_node",name=~"tools-worker-\\d\\d"})) <= 50
        for: 3m
        labels:
          severity: P2
        annotations:
          summary: Check tools workers - pool depleted by at least 50%

    - name: dev
      rules:
      - alert: dev media backup tar file size
        expr: (100 * (1 - ((backup_utility_file_size{kubernetes_cluster="dev0",type="media"} offset 24h) / (backup_utility_file_size{kubernetes_cluster="dev0",type="media"}))) < -50) or (100 * (1 - ((backup_utility_file_size{kubernetes_cluster="dev0",type="media"} offset 24h) / (backup_utility_file_size{kubernetes_cluster="dev0",type="media"}))) > 50)
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Raw dev media backup file size has increased/decreased by over 50% within an 8 day window

      - alert: dev media backup uploads (weekly)
        expr: (increase(backup_utility_upload_to_blob_count_total{kubernetes_cluster="dev0",type="media"}[8d]) OR on() vector(0)) < 1
        for: 8d
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.type }} - {{ $labels.schedule }}"
          summary: Backup tar file failed to upload from backup-scheduler pod to binkuksouthdev/backups storage account

      - alert: dev postgres backup failure
        expr: increase(backup_utility_file_count_total{job="backup-scheduler",resource=~"bink-uksouth-dev-.+",status="failed",type="postgres"}[61m]) > 1
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }} - {{ $labels.schedule }}"
          summary: Check dev backup-scheduler pod and binkuksouthdev/backups storage account - potential backup failure

      - alert: dev postgres backup sql file size
        expr: (100 * (1 - ((backup_utility_file_size{resource=~"bink-uksouth-dev-.+"} offset 24h) / (backup_utility_file_size{resource=~"bink-uksouth-dev-.+"}))) < -50) or (100 * (1 - ((backup_utility_file_size{resource=~"bink-uksouth-dev-.+"} offset 24h) / (backup_utility_file_size{resource=~"bink-uksouth-dev-.+"}))) > 50)
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Raw dev postgres backup file size has increased/decreased by over 50% within 24 hour window

      - alert: dev postgres backup uploads (hourly)
        expr: (increase(backup_utility_upload_to_blob_count_total{resource="binkuksouthdev",schedule="hourly",type="postgres"}[61m]) OR on() vector(0)) < 1
        for: 90m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.type }} - {{ $labels.schedule }}"
          summary: Backup tar file failed to upload from backup-scheduler pod to binkuksouthdev/backups storage account

      - alert: dev postgres cpu
        expr: cpu_percent_percent_max{resource_group="uksouth-dev",resource_name="bink-uksouth-dev-common"} > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.resource_name }}"
          summary: Check dev postgres - CPU over 90% for 5 minutes

      - alert: dev postgres storage
        expr: storage_percent_percent_max{resource_group="uksouth-dev",resource_name="bink-uksouth-dev-common"} > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.resource_name }}"
          summary: Check dev postgres - storage over 90% for 5 minutes

      - alert: dev postgres up state
        expr: up{job="postgres",instance="bink-uksouth-dev-common"} < 1
        for: 3m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.instance }}"
          summary: Check dev postgres - bad upstate for over 3 minutes

      - alert: dev redis cpu
        expr: serverload_percent_max{resource_group="uksouth-dev",resource_name="bink-uksouth-dev-common"} > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.resource_name }}"
          summary: Check dev redis - CPU over 90% for over 5 minutes

      - alert: dev redis memory
        expr: 100 * (usedmemory_bytes_max{resource_group="uksouth-dev",resource_name="bink-uksouth-dev-common"} / 1000000000) > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.resource_name }}"
          summary: Check dev redis - memory over 90% for over 5 minutes

      - alert: dev redis up state
        expr: redis_up{instance=~"bink-uksouth-dev.+"} < 1
        for: 3m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.instance }}"
          summary: Check dev redis - bad upstate for over 3 minutes

      - alert: dev virtual machines cpu
        expr: 100 - (avg by (name) (irate(node_cpu_seconds_total{job="azure_node",env="Development",mode="idle"}[5m])) * 100) > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Check virtual machine - CPU over 90% for over 5 minutes

      - alert: dev virtual machines memory
        expr: node_memory_MemAvailable_bytes{job="azure_node",env="Development"} < 100000000
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Check virtual machine - Available memory < 100MB for over 5 minutes

      - alert: dev virtual machines root filesystem
        expr: 100 - ((node_filesystem_avail_bytes{job="azure_node",env="Development",mountpoint="/"} * 100) / node_filesystem_size_bytes{job="azure_node",env="Development",mountpoint="/"}) > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Check virtual machines - Root filesystem over 90% used for over 5 minutes

      - alert: dev virtual machines up state
        expr: up{job="azure_node",env="Development"} < 1
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Check virtual machine - bad up state for over 5 minutes

      - alert: dev0 worker pool depleted
        expr: 100 * (sum(up{job="azure_node",name=~"dev0-worker\\d\\d"}) / count(up{job="azure_node",name=~"dev0-worker\\d\\d"})) <= 60
        for: 3m
        labels:
          severity: P2
        annotations:
          summary: Check dev0 workers - pool depleted by at least 40%

      - alert: Nginx Ingress
        expr: sum(up{kubernetes_cluster="dev",kubernetes_pod_name=~"nginx-ingress-controller.+"}) < 1
        for: 1m
        labels:
          severity: P2
        annotations:
          summary: No Ingress Nginx pods running

      - alert: RabbitMQ
        expr: sum(up{kubernetes_cluster="dev",job="rabbitmq"}) < 1
        for: 1m
        labels:
          severity: P2
        annotations:
          summary: RabbitMQ not running

    - name: preprod
      rules:
      - alert: preprod media backup tar file size
        expr: (100 * (1 - ((backup_utility_file_size{kubernetes_cluster="preprod0",type="media"} offset 24h) / (backup_utility_file_size{kubernetes_cluster="preprod0",type="media"}))) < -50) or (100 * (1 - ((backup_utility_file_size{kubernetes_cluster="preprod0",type="media"} offset 24h) / (backup_utility_file_size{kubernetes_cluster="preprod0",type="media"}))) > 50)
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Raw preprod media backup file size has increased/decreased by over 50% within an 8 day window

      - alert: preprod media backup uploads (weekly)
        expr: (increase(backup_utility_upload_to_blob_count_total{kubernetes_cluster="preprod0",type="media"}[8d]) OR on() vector(0)) < 1
        for: 8d
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.type }} - {{ $labels.schedule }}"
          summary: Backup tar file failed to upload from backup-scheduler pod to binkuksouthdpreprod/backups storage account

      - alert: preprod postgres backup failure
        expr: increase(backup_utility_file_count_total{job="backup-scheduler",resource=~"bink-uksouth-preprod-.+",status="failed",type="postgres"}[61m]) > 1
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Check preprod backup-scheduler pod and binkuksouthpreprod/backups storage account - potential backup failure

      - alert: preprod postgres backup sql file size
        expr: (100 * (1 - ((backup_utility_file_size{resource=~"bink-uksouth-preprod-.+"} offset 24h) / (backup_utility_file_size{resource=~"bink-uksouth-preprod-.+"}))) < -20) or (100 * (1 - ((backup_utility_file_size{resource=~"bink-uksouth-preprod-.+"} offset 24h) / (backup_utility_file_size{resource=~"bink-uksouth-preprod-.+"}))) > 20)
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Raw preprod postgres backup file size has increased/decreased by over 20% within 24 hour window

      - alert: preprod postgres backup uploads (hourly)
        expr: (increase(backup_utility_upload_to_blob_count_total{resource="binkuksouthpreprod",schedule="hourly",type="postgres"}[61m]) OR on() vector(0)) < 1
        for: 90m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.type }} - {{ $labels.schedule }}"
          summary: Backup tar file failed to upload from backup-scheduler pod to binkuksouthpreprod/backups storage account

      - alert: preprod postgres cpu
        expr: cpu_percent_percent_max{resource_name=~"bink-uksouth-preprod-.+"} > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.resource_name }}"
          summary: Check preprod postgres - CPU over 90% for 5 minutes

      - alert: preprod postgres storage
        expr: storage_percent_percent_max{resource_name=~"bink-uksouth-preprod-.+"} > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.resource_name }}"
          summary: Check preprod postgres - storage over 90% for 5 minutes

      - alert: preprod postgres up state
        expr: up{job="postgres",instance=~"bink-uksouth-preprod-.+"} < 1
        for: 3m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.instance }}"
          summary: Check preprod postgres - bad upstate for over 3 minutes

      - alert: preprod redis cpu
        expr: serverload_percent_max{resource_name=~"bink-uksouth-preprod-.+"} > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.resource_name }}"
          summary: Check preprod redis - CPU over 90% for over 5 minutes

      - alert: preprod redis memory
        expr: 100 * (usedmemory_bytes_max{resource_name=~"bink-uksouth-preprod-.+"} / 1000000000) > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.resource_name }}"
          summary: Check preprod redis - memory over 90% for over 5 minutes

      - alert: preprod redis up state
        expr: redis_up{instance=~"bink-uksouth-preprod.+"} < 1
        for: 3m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.instance }}"
          summary: Check preprod redis - bad upstate for over 3 minutes

      - alert: preprod virtual machines cpu
        expr: 100 - (avg by (name) (irate(node_cpu_seconds_total{job="azure_node",env="Pre-Production",mode="idle"}[5m])) * 100) > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Check virtual machine - CPU over 90% for over 5 minutes

      - alert: preprod virtual machines memory
        expr: node_memory_MemAvailable_bytes{job="azure_node",env="Pre-Production"} < 100000000
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Check virtual machine - Available memory < 100MB for over 5 minutes

      - alert: preprod virtual machines root filesystem
        expr: 100 - ((node_filesystem_avail_bytes{job="azure_node",env="Pre-Production",mountpoint="/"} * 100) / node_filesystem_size_bytes{job="azure_node",env="Pre-Production",mountpoint="/"}) > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Check virtual machines - Root filesystem over 90% used for over 5 minutes

      - alert: preprod virtual machines up state
        expr: up{job="azure_node",env="Pre-Production"} < 1
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Check virtual machine - bad up state for over 5 minutes

      - alert: preprod0 worker pool depleted
        expr: 100 * (sum(up{job="azure_node",name=~"preprod0-worker\\d\\d"}) / count(up{job="azure_node",name=~"preprod0-worker\\d\\d"})) <= 60
        for: 3m
        labels:
          severity: P2
        annotations:
          summary: Check preprod0 workers - pool depleted by at least 40%

    - name: prod
      rules:
      - alert: prod media backup tar file size
        expr: (100 * (1 - ((backup_utility_file_size{kubernetes_cluster="prod0",type="media"} offset 24h) / (backup_utility_file_size{kubernetes_cluster="prod0",type="media"}))) < -50) or (100 * (1 - ((backup_utility_file_size{kubernetes_cluster="prod0",type="media"} offset 24h) / (backup_utility_file_size{kubernetes_cluster="prod0",type="media"}))) > 50)
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Raw prod media backup file size has increased/decreased by over 50% within an 8 day window

      - alert: prod media backup uploads (weekly)
        expr: (increase(backup_utility_upload_to_blob_count_total{kubernetes_cluster="prod0",type="media"}[8d]) OR on() vector(0)) < 1
        for: 8d
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.type }} - {{ $labels.schedule }}"
          summary: Backup tar file failed to upload from backup-scheduler pod to binkuksouthprod/backups storage account

      - alert: prod postgres backup failure
        expr: increase(backup_utility_file_count_total{job="backup-scheduler",resource=~"bink-uksouth-prod-.+",status="failed",type="postgres"}[61m]) > 1
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }} - {{ $labels.schedule }}"
          summary: Check prod backup-scheduler pod and binkuksouthprod/backups storage account - potential backup failure

      - alert: prod postgres backup sql file size
        expr: (100 * (1 - ((backup_utility_file_size{resource=~"bink-uksouth-prod-.+"} offset 24h) / (backup_utility_file_size{resource=~"bink-uksouth-prod-.+"}))) < -50) or (100 * (1 - ((backup_utility_file_size{resource=~"bink-uksouth-prod-.+"} offset 24h) / (backup_utility_file_size{resource=~"bink-uksouth-prod-.+"}))) > 50)
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Raw prod postgres backup file size has increased/decreased by over 50% within 24 hour window

      - alert: prod postgres backup uploads (hourly)
        expr: (increase(backup_utility_upload_to_blob_count_total{resource="binkuksouthprod",schedule="hourly",type="postgres"}[61m]) OR on() vector(0)) < 1
        for: 90m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.type }} - {{ $labels.schedule }}"
          summary: Backup tar file failed to upload from backup-scheduler pod to binkuksouthprod/backups storage account

      - alert: prod postgres cpu
        expr: cpu_percent_percent_max{resource_name=~"bink-uksouth-prod-.+"} > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.resource_name }}"
          summary: Check prod postgres - CPU over 90% for 5 minutes

      - alert: prod postgres storage
        expr: storage_percent_percent_max{resource_name=~"bink-uksouth-prod-.+"} > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.resource_name }}"
          summary: Check prod postgres - storage over 90% for 5 minutes

      - alert: prod postgres up state
        expr: up{job="postgres",instance=~"bink-uksouth-prod-.+"} < 1
        for: 3m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.instance }}"
          summary: Check prod postgres - bad upstate for over 3 minutes

      - alert: prod redis cpu
        expr: serverload_percent_max{resource_name=~"bink-uksouth-prod-.+"} > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.resource_name }}"
          summary: Check prod redis - CPU over 90% for over 5 minutes

      - alert: prod redis memory
        expr: 100 * (usedmemory_bytes_max{resource_name=~"bink-uksouth-prod-.+"} / 1000000000) > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.resource_name }}"
          summary: Check prod redis - memory over 90% for over 5 minutes

      - alert: prod redis up state
        expr: redis_up{instance=~"bink-uksouth-prod.+"} < 1
        for: 3m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.instance }}"
          summary: Check prod redis - bad upstate for over 3 minutes

      - alert: Harmonia - Failed exports 5 hours
        expr: failed_requests_total > 1
        for: 5h
        labels:
          serverity: P2
        annotations:
          resource: "{{ $labels.view }} - {{ $labels.status }}"
          summary: There have been failed exports from harmonia for 5 hours.
      
      - alert: Harmonia - Failed exports 24 hours
        expr: failed_requests_total > 1
        for: 24h
        labels:
          serverity: P2
        annotations:
          resource: "{{ $labels.view }} - {{ $labels.status }}"
          summary: There have been failed exports from harmonia for 24 hours.

      - alert: Hermes - Http response codes - More than five 400 codes in 2 mins
        expr: sum(rate(django_http_responses_total_by_status_view_method_total{status=~"4\\d\\d", kubernetes_cluster="prod0"} [2m])) > 5
        for: 1m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.view }} - {{$labels.status }}"
          summary: More than five 400 Http responses returning in 1 min
      
      - alert: Hermes - Http response codes - More than five 500 codes in 2 mins
        expr: sum(rate(django_http_responses_total_by_status_view_method_total{status=~"5\\d\\d", kubernetes_cluster="prod0"} [2m])) > 5
        for: 1m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.view }} - {{$labels.status }}"
          summary: More than five 500 Http responses returning in 1 min

      - alert: Hermes - Http response latency - Greater than 1 second
        expr: histogram_quantile(0.99, sum by(view, le)  (rate(django_http_requests_latency_seconds_by_view_method_bucket{job="hermes", view=~"membership-plan.*|service|membership-card.*|payment-card.*|register_user|login", kubernetes_cluster="prod0"}[2m]))) > 1
        for: 3m
        labels:
          severity: P2 - Monitoring
        annotations:
          resource: "{{ $labels.view }}"
          summary: Http responses are returning is over a second for more than 3 minutes
      
      - alert: Hermes - Http response latency - Greater than 5 second
        expr: histogram_quantile(0.99, sum by(view, le)  (rate(django_http_requests_latency_seconds_by_view_method_bucket{job="hermes", view=~"membership-plan.*|service|membership-card.*|payment-card.*|register_user|login", kubernetes_cluster="prod0"}[2m]))) > 5
        for: 3m
        labels:
          severity: P2 - Monitoring
        annotations:
          resource: "{{ $labels.view }}"
          summary: Http responses are returning is over five seconds for more than 3 minutes

      - alert: Hermes - No 2XX Barclays traffic detected for over 10 minutes (8am - 8pm only)
        expr: sum(rate(django_http_responses_total_by_status_view_method_total{channel="com.barclays.bmb", kubernetes_cluster="prod0", status=~"2\\d\\d"} [5m])) == 0 and on() europe_london_hour >= 8 < 20
        for: 10m
        labels:
          severity: P2
        annotations:
          summary: No 2XX codes have returned for Barclays for 10 minutes

      - alert: Hermes - No 2XX Barclays traffic detected for over 1 hour
        expr: sum(rate(django_http_responses_total_by_status_view_method_total{channel="com.barclays.bmb", kubernetes_cluster="prod0", status=~"2\\d\\d"} [5m])) == 0
        for: 1h
        labels:
          severity: P1
        annotations:
          summary: No 2XX codes have returned for Barclays for 1 hour

      - alert: Hermes - Payment cards stuck increase
        expr: sum(hermes_current_payment_card_pending_overdue_total{kubernetes_cluster="prod0"}) > sum(hermes_current_payment_card_pending_overdue_total{kubernetes_cluster="prod0"} offset 24h +15)
        for: 5m
        labels:
          severity: P2 - Monitoring
        annotations:
          summary: Payment cards have been stuck in a pending state for more than 24 hours
      
      - alert: Loyalty scheme - Harvey Nichols files not received
        expr: (blobstorage_files_total{type="scheme",name="harvey-nichols"} OR on() vector(0)) < 1
        for: 24h
        labels:
          severity: P2 - Monitoring
        annotations:
          resource: "{{ $labels.name }} - {{ $labels.type }}"
          summary: Harvey Nichols files haven't been received in over 24 hours

      - alert: Loyalty scheme - Iceland files not received (Monday - Friday only)
        expr: (blobstorage_files_total{type="scheme",name="iceland"} OR on() vector(0)) < 1 and on() day_of_week() > 0 < 6
        for: 24h
        labels:
          severity: P2 - Monitoring
        annotations:
          resource: "{{ $labels.name }} - {{ $labels.type }}"
          summary: Iceland files haven't been received in over 24 hours

      - alert: Loyalty scheme - Wasabi files not received (12pm - 8pm only)
        expr: (increase(blobstorage_files_total{name="wasabihack"}[46m]) or on() vector(0)) < 1 and on() europe_london_hour >= 12 < 20
        for: 3m
        labels:
          severity: P2 - Monitoring
        annotations:
          resource: "{{ $labels.name }}"
          summary: Three missing files over 45 minute period - check latest Wasabi import (Transaction log lands every 15 minutes between the hours of 12pm and 8pm)

      - alert: Payment card - Amex files not received
        expr: (blobstorage_files_total{type="payment", name="amex"} OR on() vector(0)) < 1
        for: 24h
        labels:
          severity: P2 - Monitoring
        annotations:
          resource: "{{ $labels.name }} - {{ $labels.type }}"
          summary: Payment files for amex haven't been received in over 24 hours
      
      - alert: Payment card - Mastercard files not received
        expr: (blobstorage_files_total{type="payment", name="mastercard"} OR on() vector(0)) < 1
        for: 27h
        labels:
          severity: P2 - Monitoring
        annotations:
          resource: "{{ $labels.name }} - {{ $labels.type }}"
          summary: Payment files for mastercard haven't been received in over 27 hours
          
      - alert: prod virtual machines cpu
        expr: 100 - (avg by (name) (irate(node_cpu_seconds_total{job="azure_node",env="Production",mode="idle"}[5m])) * 100) > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Check virtual machine - CPU over 90% for over 5 minutes

      - alert: prod virtual machines memory
        expr: node_memory_MemAvailable_bytes{job="azure_node",env="Production"} < 100000000
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Check virtual machine - Available memory < 100MB for over 5 minutes

      - alert: prod virtual machines root filesystem
        expr: 100 - ((node_filesystem_avail_bytes{job="azure_node",env="Production",mountpoint="/"} * 100) / node_filesystem_size_bytes{job="azure_node",env="Production",mountpoint="/"}) > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Check virtual machines - Root filesystem over 90% used for over 5 minutes

      - alert: prod virtual machines up state
        expr: up{job="azure_node",env="Production"} < 1
        for: 2m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Check virtual machine - bad up state for over 2 minutes

      - alert: prod0 worker pool depleted
        expr: 100 * (sum(up{job="azure_node",name=~"prod0-worker\\d\\d"}) / count(up{job="azure_node",name=~"prod0-worker\\d\\d"})) <= 80
        for: 3m
        labels:
          severity: P1
        annotations:
          summary: Check prod0 workers - pool depleted by at least 20%
      
      - alert: Midas - Failed HTTP requests to merchants
        expr: sum(increase(request_fail_total{kubernetes_cluster="prod0"}[1h])) by (error, slug) > 6
        for: 1m
        labels:
          serverity: P2
        annotations:
          resource: "{{ $labels.slug }} - {{ $labels.error }}"
          summary: "There has been an increase in failed HTTP requests from Midas to this merchant"
      
      - alert: VOP status stuck in transitory state
        expr: sum(increase(hermes_current_vop_activation_status_total{kubernetes_cluster="prod0", status=~"Deactivating|Activating"}[10m])) by (status) > 1
        for: 1h
        labels:
          severity: P2
        annotations:
          summary: "VOP account status in transitory state for over 1 Hour"

    - name: staging
      rules:
      - alert: staging media backup tar file size
        expr: (100 * (1 - ((backup_utility_file_size{kubernetes_cluster="staging0",type="media"} offset 24h) / (backup_utility_file_size{kubernetes_cluster="staging0",type="media"}))) < -50) or (100 * (1 - ((backup_utility_file_size{kubernetes_cluster="staging0",type="media"} offset 24h) / (backup_utility_file_size{kubernetes_cluster="staging0",type="media"}))) > 50)
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Raw staging media backup file size has increased/decreased by over 50% within an 8 day window

      - alert: staging media backup uploads (weekly)
        expr: (increase(backup_utility_upload_to_blob_count_total{kubernetes_cluster="staging0",type="media"}[8d]) OR on() vector(0)) < 1
        for: 8d
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.type }} - {{ $labels.schedule }}"
          summary: Backup tar file failed to upload from backup-scheduler pod to binkuksouthstaging/backups storage account

      - alert: staging postgres backup failure
        expr: increase(backup_utility_file_count_total{job="backup-scheduler",resource=~"bink-uksouth-staging-.+",status="failed",type="postgres"}[61m]) > 1
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Check staging backup-scheduler pod and binkuksouthstaging/backups storage account - potential backup failure

      - alert: staging postgres backup sql file size
        expr: (100 * (1 - ((backup_utility_file_size{resource=~"bink-uksouth-staging-.+"} offset 24h) / (backup_utility_file_size{resource=~"bink-uksouth-staging-.+"}))) < -20) or (100 * (1 - ((backup_utility_file_size{resource=~"bink-uksouth-staging-.+"} offset 24h) / (backup_utility_file_size{resource=~"bink-uksouth-staging-.+"}))) > 20)
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Raw staging postgres backup file size has increased/decreased by over 20% within 24 hour window

      - alert: staging postgres backup uploads (hourly)
        expr: (increase(backup_utility_upload_to_blob_count_total{resource="binkuksouthstaging",schedule="hourly",type="postgres"}[61m]) OR on() vector(0)) < 1
        for: 90m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.type }} - {{ $labels.schedule }}"
          summary: Backup tar file failed to upload from backup-scheduler pod to binkuksouthstaging/backups storage account

      - alert: staging postgres cpu
        expr: cpu_percent_percent_max{resource_name="bink-uksouth-staging-common"} > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.resource_name }}"
          summary: Check staging postgres - CPU over 90% for 5 minutes

      - alert: staging postgres storage
        expr: storage_percent_percent_max{resource_name="bink-uksouth-staging-common"} > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.resource_name }}"
          summary: Check staging postgres - storage over 90% for 5 minutes

      - alert: staging postgres up state
        expr: up{job="postgres",instance="bink-uksouth-staging-common"} < 1
        for: 3m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.instance }}"
          summary: Check staging postgres - bad upstate for over 3 minutes

      - alert: staging redis cpu
        expr: serverload_percent_max{resource_name="bink-uksouth-staging-common"} > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.resource_name }}"
          summary: Check staging redis - CPU over 90% for over 5 minutes

      - alert: staging redis memory
        expr: 100 * (usedmemory_bytes_max{resource_name="bink-uksouth-staging-common"} / 1000000000) > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.resource_name }}"
          summary: Check staging redis - memory over 90% for over 5 minutes

      - alert: staging redis up state
        expr: redis_up{instance=~"bink-uksouth-staging.+"} < 1
        for: 3m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.instance }}"
          summary: Check staging redis - bad upstate for over 3 minutes

      - alert: staging virtual machines cpu
        expr: 100 - (avg by (name) (irate(node_cpu_seconds_total{job="azure_node",env="Staging",mode="idle"}[5m])) * 100) > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Check virtual machine - CPU over 90% for over 5 minutes

      - alert: staging virtual machines memory
        expr: node_memory_MemAvailable_bytes{job="azure_node",env="Staging"} < 100000000
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Check virtual machine - Available memory < 100MB for over 5 minutes

      - alert: staging virtual machines root filesystem
        expr: 100 - ((node_filesystem_avail_bytes{job="azure_node",env="Staging",mountpoint="/"} * 100) / node_filesystem_size_bytes{job="azure_node",env="Staging",mountpoint="/"}) > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Check virtual machines - Root filesystem over 90% used for over 5 minutes

      - alert: staging virtual machines up state
        expr: up{job="azure_node",env="Staging"} < 1
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Check virtual machine - bad up state for over 5 minutes

      - alert: staging0 worker pool depleted
        expr: 100 * (sum(up{job="azure_node",name=~"staging0-worker\\d\\d"}) / count(up{job="azure_node",name=~"staging0-worker\\d\\d"})) <= 60
        for: 3m
        labels:
          severity: P2
        annotations:
          summary: Check staging0 workers - pool depleted by at least 40%

    - name: sandbox
      rules:
      - alert: oat postgres cpu
        expr: cpu_percent_percent_max{resource_name="bink-uksouth-oat-common"} > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.resource_name }}"
          summary: Check oat postgres - CPU over 90% for 5 minutes

      - alert: oat postgres storage
        expr: storage_percent_percent_max{resource_name="bink-uksouth-oat-common"} > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.resource_name }}"
          summary: Check oat postgres - storage over 90% for 5 minutes

      - alert: oat postgres up state
        expr: up{job="postgres",instance="bink-uksouth-oat-common"} < 1
        for: 3m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.instance }}"
          summary: Check oat postgres - bad upstate for over 3 minutes

      - alert: oat redis cpu
        expr: serverload_percent_max{resource_name="bink-uksouth-oat-common"} > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.resource_name }}"
          summary: Check oat redis - CPU over 90% for over 5 minutes

      - alert: oat redis memory
        expr: 100 * (usedmemory_bytes_max{resource_name="bink-uksouth-oat-common"} / 1000000000) > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.resource_name }}"
          summary: Check oat redis - memory over 90% for over 5 minutes

      - alert: oat redis up state
        expr: redis_up{instance=~"bink-uksouth-oat.+"} < 1
        for: 3m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.instance }}"
          summary: Check oat redis - bad upstate for over 3 minutes
    
      - alert: oat0 worker pool depleted
        expr: 100 * (sum(up{job="azure_node",name=~"oat0-worker\\d\\d"}) / count(up{job="azure_node",name=~"oat0-worker\\d\\d"})) <= 50
        for: 3m
        labels:
          severity: P2
        annotations:
          summary: Check oat0 workers - pool depleted by at least 50%

      - alert: sandbox virtual machines cpu
        expr: 100 - (avg by (name) (irate(node_cpu_seconds_total{job="azure_node",env=~"Barclays .+|Performance",mode="idle"}[5m])) * 100) > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Check virtual machine - CPU over 90% for over 5 minutes

      - alert: sandbox virtual machines memory
        expr: node_memory_MemAvailable_bytes{job="azure_node",env=~"Barclays .+|Performance"} < 100000000
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Check virtual machine - Available memory < 100MB for over 5 minutes

      - alert: sanbox virtual machines root filesystem
        expr: 100 - ((node_filesystem_avail_bytes{job="azure_node",env=~"Barclays .+|Performance",mountpoint="/"} * 100) / node_filesystem_size_bytes{job="azure_node",env=~"Barclays .+|Performance",mountpoint="/"}) > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Check virtual machines - Root filesystem over 90% used for over 5 minutes

      - alert: sandbox virtual machines up state
        expr: up{job="azure_node",env=~"Barclays .+|Performance"} < 1
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.name }}"
          summary: Check virtual machine - bad up state for over 5 minutes

      - alert: sit postgres cpu
        expr: cpu_percent_percent_max{resource_name="bink-uksouth-sit-common"} > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.resource_name }}"
          summary: Check sit postgres - CPU over 90% for 5 minutes

      - alert: sit postgres storage
        expr: storage_percent_percent_max{resource_name="bink-uksouth-sit-common"} > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.resource_name }}"
          summary: Check sit postgres - storage over 90% for 5 minutes

      - alert: sit postgres up state
        expr: up{job="postgres",instance="bink-uksouth-sit-common"} < 1
        for: 3m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.instance }}"
          summary: Check sit postgres - bad upstate for over 3 minutes

      - alert: sit redis cpu
        expr: serverload_percent_max{resource_name="bink-uksouth-sit-common"} > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.resource_name }}"
          summary: Check sit redis - CPU over 90% for over 5 minutes

      - alert: sit redis memory
        expr: 100 * (usedmemory_bytes_max{resource_name="bink-uksouth-sit-common"} / 1000000000) > 90
        for: 5m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.resource_name }}"
          summary: Check sit redis - memory over 90% for over 5 minutes

      - alert: sit redis up state
        expr: redis_up{instance=~"bink-uksouth-sit.+"} < 1
        for: 3m
        labels:
          severity: P2
        annotations:
          resource: "{{ $labels.instance }}"
          summary: Check sit redis - bad upstate for over 3 minutes

      - alert: sit0 worker pool depleted
        expr: 100 * (sum(up{job="azure_node",name=~"sit0-worker\\d\\d"}) / count(up{job="azure_node",name=~"sit0-worker\\d\\d"})) <= 50
        for: 3m
        labels:
          severity: P2
        annotations:
          summary: Check sit0 workers - pool depleted by at least 50%

  prometheus.yml: |-
    global:
      scrape_interval: 1m
      evaluation_interval: 1m
    rule_files:
      - /etc/prometheus/prometheus.rules
    alerting:
      alertmanagers:
      - scheme: http
        static_configs:
        - targets:
          - "alertmanager:9093"
    scrape_configs:
      - job_name: 'azure_node'
        scheme: http
        azure_sd_configs:
          - authentication_method: ManagedIdentity  # Core
            subscription_id: 0add5c8e-50a6-4821-be0f-7a47c879b009
            tenant_id: a6e2367a-92ea-4e5a-b565-723830bcc095
            client_id: 9119949a-375d-405b-8f4c-4f44be820468
            port: 9100
          - authentication_method: ManagedIdentity  # Staging
            subscription_id: 457b0db5-6680-480f-9e77-2dafb06bd9dc
            tenant_id: a6e2367a-92ea-4e5a-b565-723830bcc095
            client_id: 9119949a-375d-405b-8f4c-4f44be820468
            port: 9100
          - authentication_method: ManagedIdentity  # Production
            subscription_id: 79560fde-5831-481d-8c3c-e812ef5046e5
            tenant_id: a6e2367a-92ea-4e5a-b565-723830bcc095
            client_id: 9119949a-375d-405b-8f4c-4f44be820468
            port: 9100
          - authentication_method: ManagedIdentity  # PreProduction
            subscription_id: 6e685cd8-73f6-4aa6-857c-04ed9b21d17d
            tenant_id: a6e2367a-92ea-4e5a-b565-723830bcc095
            client_id: 9119949a-375d-405b-8f4c-4f44be820468
            port: 9100
          - authentication_method: ManagedIdentity  # Development
            subscription_id: 794aa787-ec6a-40dd-ba82-0ad64ed51639
            tenant_id: a6e2367a-92ea-4e5a-b565-723830bcc095
            client_id: 9119949a-375d-405b-8f4c-4f44be820468
            port: 9100
          - authentication_method: ManagedIdentity  # Sandbox
            subscription_id: 957523d8-bbe2-4f68-8fae-95975157e91c
            tenant_id: a6e2367a-92ea-4e5a-b565-723830bcc095
            client_id: 9119949a-375d-405b-8f4c-4f44be820468
            port: 9100

        relabel_configs:
          - source_labels: ["__meta_azure_machine_name"]
            regex: wireguard
            action: replace
            replacement: 20.49.163.188:9100
            target_label: __address__
          - source_labels: ["__meta_azure_machine_name"]
            target_label: "name"
          - source_labels: ["__meta_azure_machine_tag_Environment"]
            target_label: "env"
          - source_labels: [name]
            regex: '0365-backup|qa-regression-testing|uksouth-mastercard-vm'
            action: drop

      - job_name: 'tools-pods'
        kubernetes_sd_configs:
          - role: pod
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name

          # Replace Job name based on attribute
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_job]
            action: replace
            target_label: job
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_instance]
            action: replace
            target_label: instance

          - source_labels: [kubernetes_cluster]
            target_label: kubernetes_cluster
            action: replace
            regex: ()
            replacement: tools

      - job_name: 'dev0-pods'
        kubernetes_sd_configs:
          - api_server: https://dev0-controller.uksouth.bink.host:6443
            role: pod
            bearer_token_file: /tmp/prometheus-creds/dev0/token
            tls_config:
              ca_file: /tmp/prometheus-creds/dev0/ca.crt
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name

          # Replace Job name based on attribute
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_job]
            action: replace
            target_label: job
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_instance]
            action: replace
            target_label: instance

          - source_labels: [kubernetes_cluster]
            target_label: kubernetes_cluster
            action: replace
            regex: ()
            replacement: dev0

      - job_name: 'oat0-pods'
        kubernetes_sd_configs:
          - api_server: https://oat0-controller.uksouth.bink.host:6443
            role: pod
            bearer_token_file: /tmp/prometheus-creds/oat0/token
            tls_config:
              ca_file: /tmp/prometheus-creds/oat0/ca.crt
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name

          # Replace Job name based on attribute
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_job]
            action: replace
            target_label: job
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_instance]
            action: replace
            target_label: instance

          - source_labels: [kubernetes_cluster]
            target_label: kubernetes_cluster
            action: replace
            regex: ()
            replacement: oat0

      - job_name: 'sit0-pods'
        kubernetes_sd_configs:
          - api_server: https://sit0-controller.uksouth.bink.host:6443
            role: pod
            bearer_token_file: /tmp/prometheus-creds/sit0/token
            tls_config:
              ca_file: /tmp/prometheus-creds/sit0/ca.crt
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name

          # Replace Job name based on attribute
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_job]
            action: replace
            target_label: job
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_instance]
            action: replace
            target_label: instance

          - source_labels: [kubernetes_cluster]
            target_label: kubernetes_cluster
            action: replace
            regex: ()
            replacement: sit0

      - job_name: 'staging0-pods'
        kubernetes_sd_configs:
          - api_server: https://staging0-controller.uksouth.bink.host:6443
            role: pod
            bearer_token_file: /tmp/prometheus-creds/staging0/token
            tls_config:
              ca_file: /tmp/prometheus-creds/staging0/ca.crt
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name

          # Replace Job name based on attribute
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_job]
            action: replace
            target_label: job
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_instance]
            action: replace
            target_label: instance

          - source_labels: [kubernetes_cluster]
            target_label: kubernetes_cluster
            action: replace
            regex: ()
            replacement: staging0

      - job_name: 'perf0-pods'
        kubernetes_sd_configs:
          - api_server: https://perf0-controller.uksouth.bink.host:6443
            role: pod
            bearer_token_file: /tmp/prometheus-creds/perf0/token
            tls_config:
              ca_file: /tmp/prometheus-creds/perf0/ca.crt
              insecure_skip_verify: true
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name

          # Replace Job name based on attribute
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_job]
            action: replace
            target_label: job
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_instance]
            action: replace
            target_label: instance

          - source_labels: [kubernetes_cluster]
            target_label: kubernetes_cluster
            action: replace
            regex: ()
            replacement: perf0

      - job_name: 'preprod0-pods'
        kubernetes_sd_configs:
          - api_server: https://preprod0-controller.uksouth.bink.host:6443
            role: pod
            bearer_token_file: /tmp/prometheus-creds/preprod0/token
            tls_config:
              ca_file: /tmp/prometheus-creds/preprod0/ca.crt
              insecure_skip_verify: true
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name

          # Replace Job name based on attribute
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_job]
            action: replace
            target_label: job
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_instance]
            action: replace
            target_label: instance

          - source_labels: [kubernetes_cluster]
            target_label: kubernetes_cluster
            action: replace
            regex: ()
            replacement: preprod0

      - job_name: 'prod0-pods'
        kubernetes_sd_configs:
          - api_server: https://prod0-controller.uksouth.bink.host:6443
            role: pod
            bearer_token_file: /tmp/prometheus-creds/prod0/token
            tls_config:
              ca_file: /tmp/prometheus-creds/prod0/ca.crt
              insecure_skip_verify: true
        relabel_configs:
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            action: keep
            regex: true
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
            action: replace
            target_label: __metrics_path__
            regex: (.+)
          - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
            action: replace
            regex: ([^:]+)(?::\d+)?;(\d+)
            replacement: $1:$2
            target_label: __address__
          - source_labels: [__meta_kubernetes_namespace]
            action: replace
            target_label: kubernetes_namespace
          - source_labels: [__meta_kubernetes_pod_name]
            action: replace
            target_label: kubernetes_pod_name

          # Replace Job name based on attribute
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_job]
            action: replace
            target_label: job
          - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_instance]
            action: replace
            target_label: instance

          - source_labels: [kubernetes_cluster]
            target_label: kubernetes_cluster
            action: replace
            regex: ()
            replacement: prod0

      - job_name: 'tools-nodes'
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

        kubernetes_sd_configs:
          - role: node
        relabel_configs:
          - target_label: __address__
            replacement: kubernetes.default.svc:443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
          - source_labels: [kubernetes_cluster]
            target_label: kubernetes_cluster
            action: replace
            regex: ()
            replacement: tools

      - job_name: 'dev0-nodes'
        scheme: https
        # All the hacks, goes to kube to get list of worker nodes
        # Then relabels to override the address and then hits the
        # API proxy endpoint for each node to get metrics
        # Hence the need to specify creds twice
        tls_config:
          ca_file: /tmp/prometheus-creds/dev0/ca.crt
        bearer_token_file: /tmp/prometheus-creds/dev0/token

        kubernetes_sd_configs:
          - api_server: https://dev0-controller.uksouth.bink.host:6443
            role: node
            bearer_token_file: /tmp/prometheus-creds/dev0/token
            tls_config:
              ca_file: /tmp/prometheus-creds/dev0/ca.crt
        relabel_configs:
          - target_label: __address__
            replacement: dev0-controller.uksouth.bink.host:6443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
          - source_labels: [kubernetes_cluster]
            target_label: kubernetes_cluster
            action: replace
            regex: ()
            replacement: dev0

      - job_name: 'oat0-nodes'
        scheme: https
        tls_config:
          ca_file: /tmp/prometheus-creds/oat0/ca.crt
        bearer_token_file: /tmp/prometheus-creds/oat0/token

        kubernetes_sd_configs:
          - api_server: https://oat0-controller.uksouth.bink.host:6443
            role: node
            bearer_token_file: /tmp/prometheus-creds/oat0/token
            tls_config:
              ca_file: /tmp/prometheus-creds/oat0/ca.crt
        relabel_configs:
          - target_label: __address__
            replacement: oat0-controller.uksouth.bink.host:6443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
          - source_labels: [kubernetes_cluster]
            target_label: kubernetes_cluster
            action: replace
            regex: ()
            replacement: oat0

      - job_name: 'sit0-nodes'
        scheme: https
        tls_config:
          ca_file: /tmp/prometheus-creds/sit0/ca.crt
        bearer_token_file: /tmp/prometheus-creds/sit0/token

        kubernetes_sd_configs:
          - api_server: https://sit0-controller.uksouth.bink.host:6443
            role: node
            bearer_token_file: /tmp/prometheus-creds/sit0/token
            tls_config:
              ca_file: /tmp/prometheus-creds/sit0/ca.crt
        relabel_configs:
          - target_label: __address__
            replacement: sit0-controller.uksouth.bink.host:6443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
          - source_labels: [kubernetes_cluster]
            target_label: kubernetes_cluster
            action: replace
            regex: ()
            replacement: sit0

      - job_name: 'staging0-nodes'
        scheme: https
        tls_config:
          ca_file: /tmp/prometheus-creds/staging0/ca.crt
        bearer_token_file: /tmp/prometheus-creds/staging0/token

        kubernetes_sd_configs:
          - api_server: https://staging0-controller.uksouth.bink.host:6443
            role: node
            bearer_token_file: /tmp/prometheus-creds/staging0/token
            tls_config:
              ca_file: /tmp/prometheus-creds/staging0/ca.crt
        relabel_configs:
          - target_label: __address__
            replacement: staging0-controller.uksouth.bink.host:6443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
          - source_labels: [kubernetes_cluster]
            target_label: kubernetes_cluster
            action: replace
            regex: ()
            replacement: staging0

      - job_name: 'perf0-nodes'
        scheme: https
        tls_config:
          ca_file: /tmp/prometheus-creds/perf0/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /tmp/prometheus-creds/perf0/token

        kubernetes_sd_configs:
          - api_server: https://perf0-controller.uksouth.bink.host:6443
            role: node
            bearer_token_file: /tmp/prometheus-creds/perf0/token
            tls_config:
              ca_file: /tmp/prometheus-creds/perf0/ca.crt
              insecure_skip_verify: true
        relabel_configs:
          - target_label: __address__
            replacement: perf0-controller.uksouth.bink.host:6443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
          - source_labels: [kubernetes_cluster]
            target_label: kubernetes_cluster
            action: replace
            regex: ()
            replacement: perf0

      - job_name: 'preprod0-nodes'
        scheme: https
        tls_config:
          ca_file: /tmp/prometheus-creds/preprod0/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /tmp/prometheus-creds/preprod0/token

        kubernetes_sd_configs:
          - api_server: https://preprod0-controller.uksouth.bink.host:6443
            role: node
            bearer_token_file: /tmp/prometheus-creds/preprod0/token
            tls_config:
              ca_file: /tmp/prometheus-creds/preprod0/ca.crt
              insecure_skip_verify: true
        relabel_configs:
          - target_label: __address__
            replacement: preprod0-controller.uksouth.bink.host:6443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
          - source_labels: [kubernetes_cluster]
            target_label: kubernetes_cluster
            action: replace
            regex: ()
            replacement: preprod

      - job_name: 'prod0-nodes'
        scheme: https
        tls_config:
          ca_file: /tmp/prometheus-creds/prod0/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /tmp/prometheus-creds/prod0/token

        kubernetes_sd_configs:
          - api_server: https://prod0-controller.uksouth.bink.host:6443
            role: node
            bearer_token_file: /tmp/prometheus-creds/prod0/token
            tls_config:
              ca_file: /tmp/prometheus-creds/prod0/ca.crt
              insecure_skip_verify: true
        relabel_configs:
          - target_label: __address__
            replacement: prod0-controller.uksouth.bink.host:6443
          - source_labels: [__meta_kubernetes_node_name]
            regex: (.+)
            target_label: __metrics_path__
            replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
          - source_labels: [kubernetes_cluster]
            target_label: kubernetes_cluster
            action: replace
            regex: ()
            replacement: prod0

      - job_name: ssl
        metrics_path: /probe
        params:
          module: ["https_noverify"]
        static_configs:
          - targets:
            - api.gb.bink.com:443
            - bink.com:443
            - git.bink.com:443
            - core.spreedly.com:443
        relabel_configs:
          - source_labels: [__address__]
            target_label: __param_target
          - source_labels: [__param_target]
            target_label: instance
          - target_label: __address__
            replacement: ssl-exporter
          - source_labels: [cert_use]
            target_label: cert_use
            action: replace
            regex: ()
            replacement: server
      
      - job_name: azurefrontdoor_dns
        metrics_path: /probe
        params:
          module: ["dns_api_gb_bink_com"]
        static_configs:
          - targets:
            - 1.1.1.1
        relabel_configs:
          - source_labels: [__address__]
            target_label: __param_target
          - source_labels: [__param_target]
            target_label: instance
          - target_label: __address__
            replacement: blackbox-exporter
      
      - job_name: wireguard
        scrape_interval: 120s
        static_configs:
          - targets:
            - 20.49.163.188:9586
      
      - job_name: aqua
        scrape_interval: 60s
        bearer_token: ab534a41deca5d73c62dcebca893930661af4abc
        scheme: https
        static_configs:
          - targets:
            - aqua.uksouth.bink.sh:443

      - job_name: checkly
        scrape_interval: 30s
        metrics_path: /accounts/da41e9cc-d34a-4cab-b7d6-44740d1c7d2b/prometheus/metrics
        bearer_token: pUZDW0IXdDBWoDCxAgJ3eRsZ
        scheme: https
        static_configs:
          - targets:
            - api.checklyhq.com:443

